{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> INF393 - Maquinas de Aprendizaje  </h1>\n",
    "    <h2> Tarea 3: Métodos No Lineales para Clasificación </h2>\n",
    "    <h5> Alumnos: Ignacio Espinoza & Daniel Rivera</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> PARTE 3: Reconocimiento de Imágenes en CIFAR 10 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>La matriz de elementos consisten en una matriz de $10000 × 3072$ , en donde cada fila de esa matriz corresponde a una imagen en formato $RGB$, y para cada imagen, o sea cada fila, los primeros $1024$ valores vienen del canal $R$, los siguientes $1024$ del canal $G$, y los últimos $1024$ del canal $B$.\n",
    "Para cada canal, las imágenes han sido vectorizadas por filas, de modo que los primeros 32 valores del canal\n",
    "R corresponden a la primera fila de la imagen. Por otro lado, el elemento (labels) del diccionario contiene\n",
    "una lista de 1000 valores enteros entre 0 y 9 que identifican las clases excluyentes  las cuales son \n",
    "<i>gato, perro, rana, caballo, pajaro, ciervo, avion, automovil, camion y barco:</i>.</p>\n",
    "\n",
    "<h4> Generando Dataset </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import imread\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def unpickle(file):\n",
    "   \n",
    "   fo = open(file, 'rb')\n",
    "   dict = pickle.load(fo)\n",
    "   fo.close()\n",
    "   return dict\n",
    "\n",
    "def load_CIFAR_one(filename):\n",
    "   with open(filename, 'rb') as f:\n",
    "      datadict = pickle.load(f)\n",
    "      X = datadict['data']\n",
    "      Y = datadict['labels']\n",
    "      return X, np.array(Y)\n",
    "\n",
    "def load_CIFAR10(PATH):\n",
    "   xs = []\n",
    "   ys = []\n",
    "   for b in range(1,6):\n",
    "      f = os.path.join(PATH, 'data_batch_%d' % (b, ))\n",
    "      X, Y = load_CIFAR_one(f)\n",
    "      xs.append(X)\n",
    "      ys.append(Y)\n",
    "   Xtr = np.concatenate(xs)\n",
    "   Ytr = np.concatenate(ys)\n",
    "   del X, Y\n",
    "   Xte, Yte = load_CIFAR_one(os.path.join(PATH, 'test_batch'))\n",
    "   \n",
    "   i = random.randint(0, 40000)\n",
    "   Xval = Xtr[i:(i+10000)]\n",
    "   Yval = Ytr[i:(i+10000)]\n",
    "\n",
    "   return Xtr, Ytr, Xte, Yte, Xval,Yval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data1(Xtr,Xte,Xval):\n",
    "   return Xtr/255.0 ,Xte/255.0 ,Xval/255.0\n",
    "\n",
    "def scale_data2(Xtr,Xte):\n",
    "   scaler = StandardScaler()\n",
    "   Xtr_scaled = scaler.fit_transform(Xtr[:20000])\n",
    "   Xte_scaled = scaler.fit_transform(Xte)\n",
    "   #Xval = scaler.fit_transform(Xval)\n",
    "   return Xtr_scaled, Xte_scaled\n",
    "\n",
    "def scaler_function(Xtr,Xte,Xv,scale=True):\n",
    "   scaler = StandardScaler(with_std = scale).fit(Xtr[:22000])\n",
    "   Xtr_scaled = scaler.transform(Xtr[:22000])\n",
    "   Xte_scaled = scaler.transform(Xte)\n",
    "   Xv_scaled = scaler.transform(Xte)\n",
    "   return Xtr_scaled, Xte_scaled , Xv_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Escalando Datos </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xte, Yte, Xval, Yval = load_CIFAR10('.')\n",
    "\n",
    "Xtr_scaled, Xte_scaled, Xval_scaled = scaler_function(Xtr,Xte,Xval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CONSIDERACION ESPECIAL POR PROBLEMAS DE MEMORIA, SE REDUCE EL DATASET\n",
    "Xtr,Xte, Xval = scaler_function(Xtr,Xte,Xval)\n",
    "Ytr = to_categorical(Ytr[:22000])\n",
    "Yte = to_categorical(Yte)\n",
    "Yval = to_categorical(Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementación : Redes Neuronales </h3>\n",
    "\n",
    "<p> Al implementar redes neuronales en un problema de clasificación múltiple, debemos generar una función de hipótesis de modo que lo que retorne sea un vector de valores, el cual será un vector de $0$ con excepción de la clase que será clasificada, asignandole un valor igual a $1$.</p>\n",
    "\n",
    "\n",
    "$$y^{(i)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\end{bmatrix} , \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\end{bmatrix} $$\n",
    "\n",
    "<img src=\"NeuralNetwork3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> ¿Cual es el mejor criterio para seleccionar cantidad de capas ocultas y neuronas por capa ? </h4>\n",
    "\n",
    "<p> No existe una regla de oro para la selección de capas ocultas y nodos , por lo que es necesario recurrir a heurísticas y pruebas de entrenamiento para encontrar la mejor configuración. De todos modos, existen ciertas convenciones, acorde a la estructua del dataset. A continuación se presentarán las heurísticas importantes:</p>\n",
    "\n",
    "<h4> Heurística #1 </h4>\n",
    "<h3>$$ N_h = \\frac{N_s}{(\\alpha * (N_i + N_o))}$$</h3>\n",
    "\n",
    "<ul>\n",
    "<li> $N_i$ = número de neuronas de entrada</li>\n",
    "<li> $N_o$ = número de neuronas de salida</li>\n",
    "<li> $N_s$ = número de elementos en el dataset</li>\n",
    "<li> $ \\alpha$ = valor arbitrario entre 5-10</li>\n",
    "</ul>\n",
    "\n",
    "<p> Con esta heurística, se obtiene un valor de 3 nodos en una capa oculta, por lo que a continuación entrenará una red neuronal con 1 capa oculta y 3 nodos </p>\n",
    "\n",
    "<h4>Heurística #2:</h4>\n",
    "<p>\"1 Capa Oculta es Suficiente\"</p>\n",
    "<p>Elegir un valor promedio entre número neuronas de entrada y salida</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_classifier(Xtr, Ytr, Xte, Yte, Xv, Yv):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim = Xtr.shape[1], init = 'uniform', activation = 'sigmoid'))\n",
    "    model.add( Dropout(0.1) )\n",
    "    model.add(Dense(10, init = 'uniform', activation = 'softmax'))\n",
    "    model.compile(optimizer = SGD( lr = 0.05), loss = 'binary_crossentropy', metrics = ['accuracy'] )\n",
    "    model.fit(Xtr, Ytr, nb_epoch = 50, batch_size = 32, verbose = 1, validation_data = (Xte,Yte))\n",
    "    scores = model.evaluate(Xte, Yte)\n",
    "    test_acc = scores[1]\n",
    "    scores = model.evaluate(Xv, Yv)\n",
    "    val_acc = scores[1]\n",
    "    scores = model.evaluate(Xtr, Ytr)\n",
    "    train_acc = scores[1]\n",
    "    \n",
    "    print \"Test Accuracy: \" + str(test_acc)\n",
    "    \n",
    "\n",
    "    print \"Train Accuracy: \" + str(train_acc)\n",
    "    \n",
    "    \n",
    "    print \"Evaluate Accuracy: \" + str(val_acc)\n",
    "\n",
    "#neural_network_classifier(Xtr, Ytr, Xte, Yte, Xval, Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3> RELU 55 NODOS</h3>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Experimento </th>\n",
    "<th> Training </th>\n",
    "<th>Test</th>\n",
    "<th>Eval </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>SIGMOID 2 CAPAS  50 NODOS</th>\n",
    "<td>0.900209069772</td>\n",
    "<td>0.90017997818</td>\n",
    "<td>0.900379990196</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>SIGMOID 100 NODOS</th>\n",
    "<td> 0.911720000458</td>\n",
    "<td>0.919250000217</td>\n",
    "<td> 0.895939997864</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>RELU 100 NODOS s</th>\n",
    "<td>0.960490908059</td>\n",
    "<td>0.909230000687</td>\n",
    "<td>0.858430000114</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>SIGMOID 55 NODOS</th>\n",
    "<td>0.910940000439</td>\n",
    "<td>0.916822728157</td>\n",
    "<td> 0.894919999218</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>Sigmoid 2 NO DROPOUT</th>\n",
    "<td>0.900081810518</td>\n",
    "<td>899899991989</td>\n",
    "<td>0.83709999885</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>1 CAPAS , 3 NODOS</th>\n",
    "<td>0.900099984776</td>\n",
    "<td>0.89994998455</td>\n",
    "<td>0.830809999466</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>1 CAPAS, 100 NODOS</th>\n",
    "<td>0.914449999289</td>\n",
    "<td>0.907350000858</td>\n",
    "<td> 0.9144499992896</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "Como se observa, el mejor resultado se obtuvo con 55 nodos, pero tambien usando 2 capas y 50 nodos se obtiene un accurace del 90%. Este resultado no determina ninguna correlación entre el número de nodos y el número de capas ocultas y la distribución existente entre ellas.\n",
    "\n",
    "Por otro lado, la función sigmoid, en comparación con Relu, mostro resultados peores, esto refleja la estructura dispersa de los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from top_level_features import hog_features\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7f985271cc5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Xtr, Ytr, Xte, Yte, Xval, Yval = load_CIFAR10(\".\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolor_histogram_hsv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#extrae histogramas de color\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolor_histogram_hsv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#extrae histogramas de color\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolor_histogram_hsv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/Dropbox/UTFSM/Machine Learning/Tarea/Tarea3/top_level_features.pyc\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(array_imgs, feature_fns, verbose)\u001b[0m\n\u001b[0;32m     27\u001b[0m   \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \"\"\"\n\u001b[1;32m---> 29\u001b[1;33m   \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m   \u001b[1;32mprint\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m   \u001b[0mnum_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Xtr, Ytr, Xte, Yte, Xval, Yval = load_CIFAR10(\".\")\n",
    "\n",
    "train_features = extract_features(Xtr,[color_histogram_hsv]) #extrae histogramas de color\n",
    "test_features = extract_features(Xte,[color_histogram_hsv]) #extrae histogramas de color\n",
    "val_features = extract_features(Xv,[color_histogram_hsv])\n",
    "                                \n",
    "train_features , test_features, val_features = scaler_function(train_feature,test_features,val_features)\n",
    "\n",
    "\n",
    "#neural_network_classifier(train_features, Ytr, test_features, Yte, val_features, Yval)\n",
    "#ERROR DE MEMORIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementación SVM No Lineal </h3>\n",
    "Es importante considerar que con SVM No Lineal, la  implementación de <i>sklearn</i> funciona correctamente con datasets superior a los 10000 elementos, por lo que se debe reducir la dimensionalidad del set de entrenamiento. Esto puede conllevar a un sobreajusta dada la poca cantidad de datos relativo al resto de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NoLineal_SVM_classifier(Xtr, Ytr, Xte, Yte, Xv, Yv):\n",
    "    print (\"hola\")\n",
    "    clf_kernel_rbf = SVC(C = 500, kernel = 'rbf')\n",
    "    clf_kernel_rbf.fit(Xtr, Ytr)\n",
    "\n",
    "    #clf_kernel_poly = SVC(C = 10, kernel = 'poly', degree = 2, coef0 = 1)\n",
    "    #clf_kernel_poly.fit(Xtr, Ytr)\n",
    "    \n",
    "    #training_kernel_score = clf_kernel_rbf.score(Xtr,Ytr)\n",
    "    \n",
    "    testing_kernel_score = clf_kernel_rbf.score(Xte,Yte)\n",
    "    \n",
    "    eval_kernel_score = clf_kernel_rbf.score(Xv,Yv)\n",
    "    \n",
    "    #training_poly_score = clf_kernel_poly.score(Xtr,Ytr)\n",
    "    \n",
    "    #testing_poly_score = clf_kernel_poly.score(Xte,Yte)\n",
    "    \n",
    "    #eval_poly_score = clf_kernel_poly.score(Xv,Yv)\n",
    "    \n",
    "    #print \"Kernel Training Accuracy: \" + str(training_kernel_score)\n",
    "    \n",
    "    print \"Kernel Test Accuracy: \" + str(testing_kernel_score)\n",
    "    \n",
    "    print \"Kernel Eval Accuracy: \" + str(eval_kernel_score)\n",
    "    \n",
    "    #print \"Polynomial Training Accuracy: \" + str(training_poly_score)\n",
    "    \n",
    "    #print \"Polynomial Training Accuracy: \" + str(testing_poly_score)\n",
    "    \n",
    "    #print \"Polynomial Training Accuracy: \" + str(eval_poly_score)\n",
    "    \n",
    "\n",
    "NoLineal_SVM_classifier(Xtr[:100], Ytr[:100], Xte[:100], Yte[:100], Xval[:100], Yval[:100])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Kernel Training Accuracy: 1.0 </h4>\n",
    "<h4> Kernel Test Accuracy: 0.1  </h4>\n",
    "<h4>Kernel Eval Accuracy: 0.097</h4>\n",
    "\n",
    "Implementar un SVM No Lineal a un dataset de alta dimensionalidad conlleva a un gasto de recursos y tiempo elevado, a su vez, los resultados no son alentadores, obtiendo en general un accuracy cercano al 10%.Estos datos se obtuvieron con un coeficiente de regularización alto, o sea mas ajustado al set de entrenamiento, por lo que se observa overfitting. El mejor resultado se obtiene con un C cercano a 10 (menos sensible a ruido) con un accuracy en evaluación cercano al 40%. Se concluye que los SVM no lineales no logran generar un model eficiente para el problema de reconocimiento de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
